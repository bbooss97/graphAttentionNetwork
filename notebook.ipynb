{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH ATTENTION NETWORKS\n",
    "\n",
    "The following is the reimplementation and reproduction of results of the paper Graph Attention Networks by Petar Velickovic,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro Li√≤,Yoshua Bengio.\n",
    "\n",
    "What is a graph attention network?\n",
    "This neural network architecture operates on graph data and leveraging on graph convolutional networks and self attention layers allows to improve the accuracy of previous techniques on common graph datasets including the cora dateset,citeseet,pubmed and ppi.\n",
    "\n",
    "Graph convolutional layer:\n",
    "\n",
    "In the case of graph data the input is a graph with nodes that have a n dimensional vector representing its features and an adiacency matrix representing the connections between the different nodes of the graph.\n",
    "In the case of graph convolutional layer to produce the hidden representation of a node x we would apply a sort of message passing ,where there are parameters to optimize ,between the nodes x and its neighbors, and this operation is done in parallel in all the nodes. \n",
    "The nodes features are all stacked in a matrix and this matrix is used to derive the hidden features for all the nodes using a W matrix of learnable weights and the adiacency matrix.\n",
    "The weight matrix is where the learning takes place and could be descibed as a matrix or seen as a fully connected layer on the node embeddings.\n",
    "The nodes features are obtained from the summation over all the neighbors features that are derived from the matrix multiplication with the weihgt matrix\n",
    "The general formula to obtain the hidden features at layer i is:\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Where W is the weight matrix,hj are the node embeddings from the last layer,N is the index of the neighbors of the node i and sigma is an activation function.\n",
    "The aggregation function at the end can be a simple sum or for example an average or a different type of formula even one with learnable parameters.\n",
    "In practise all of those operations are done in parallel using matrix multiplication and gpu for example in the pytorch geometric package and can be summarized as multiplying the features nodes with the weight matrix and then with the adiacency matrix and at the final step apply an activation function.\n",
    "\n",
    "With convolutional layers we obtain a sort of message passing with learning parameters and the messages comes from the neighbors, thats why the number of convolutional layers is important because applying n convolutional layers allows to connect nodes n edges apart.\n",
    "\n",
    "Self attention:\n",
    "The convolutional layer can be estended using self attention.\n",
    "One of the most important features of the graph attention network is the use of self attention, a mechanism commonly used in state of the art models in natural language processing , capable of learning how to give attention to different parts of the inputs to give the best possible results. \n",
    "The node i in this case not only has to work with its neighbors as in the graph convolutional layers but has also to learn attention coefficients to give to the neighboors of itself, to weight the features coming from them.\n",
    "\n",
    "The attention coefficent that nodes i gives to node j can be seen as \n",
    "\\begin{equation}\n",
    "e_{i j}=a\\left(W h_{i}, W h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where W is the parameters matrix and hi and hj are the nodes embeddings.\n",
    "\n",
    "Before describing how the attention function a is implemented lets describe the normalization applied to the coefficients of node i neighboors:\n",
    "this is the formula used to normaliza that is a simple softmax in order to normalize the coefficients of the neighbors to have a value between 0 and 1 and weightd by their importance :\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\operatorname{softmax}_{j}\\left(e_{i j}\\right)=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k e N(i)} \\exp \\left(e_{i k}\\right)}=\\frac{\\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}{\\sum_{k e N(i)} \\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the softmax function firstly exponentiates all the eij attention coefficients to obtain values between 0 and 1 and then returns the percentage of its attention with respect to the attention given to all the neighboors togheter.\n",
    "\n",
    "Now let's see how the attention coefficients aij are calculated.\n",
    "There are different ways to get the attention coefficients, for example in the original paper attention is all you neeed its calculated by using the dot product of the features learned applying the a linear trasformation to the embeddings.\n",
    "\n",
    "However in this paper they opted for a different type of attention mechanism:\n",
    "\n",
    "they firstly stack the features obtained by multiplying the embeddings with W and then they apply a fully connected layer to get a value that is then passed by a leaky relu activation function before the softmax normalization.\n",
    "The formula applied at the end to obtain the coefficients normalized is the follwoing:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\frac{\\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}{\\sum_{k \\in N(i)} \\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the LeakyReLu is used to cut off all the negatives values by a large amount but the entire one and is helpful to emphatize positive relationships found by the model.\n",
    "\n",
    "graph attention network:\n",
    "\n",
    "merging the things seen before allow to implement the graph attention layer where there is the use of the graph convolutional layer and the attention coefficients to weight the neighbors messages like a linear combination.\n",
    "Here is the formula that describes how to obtain the node features:\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} \\alpha_{i j} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "the aij are values between 0 and 1 that sums to one and are applied in the summation before the activation function\n",
    "\n",
    "To summarize the learnable weights are the ones of the matrix W used to transform the embeddings and the ones of the attention mechanism (fully connected layer).\n",
    "In practise we use multihead attention ,so we learn multiple attention coefficients between i and j and we use those to weight the message passing of the neightboors and obtain the features of the output.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#this is the graph attention network module\n",
    "class GraphAttentionNetwork(torch.nn.Module):\n",
    "  \"\"\"Graph Attention Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "    super().__init__()\n",
    "    #we have 2 graph attention layers specifying dimension of inputs,outputs and heads\n",
    "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "  def forward(self, data):\n",
    "    #i get the input features and the graph connections as input x\n",
    "    x, edge_index=data.x,data.edge_index\n",
    "    #i filter the input features with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(x, p=0.6, training=self.training)\n",
    "    #i apply the first graph attention layer\n",
    "    h = self.gat1(x, edge_index)\n",
    "    #i apply a elu activation function as in the paper\n",
    "    h = F.elu(h)\n",
    "    #i filter the features h with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(h, p=0.6, training=self.training)\n",
    "    #i apply the second graph attention layer\n",
    "    h = self.gat2(h, edge_index)\n",
    "    #i return the logsoftmax of the output to normalize the output\n",
    "    return h\n",
    "\n",
    "#this is the graph convolutional network module\n",
    "class GraphConvolutionalNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.conv1 = GCNConv(in_size, h_size)\n",
    "        self.conv2 = GCNConv(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.fc1 = torch.nn.Linear(in_size, h_size)\n",
    "        self.fc2 = torch.nn.Linear(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.fc1(x)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.fc2(x)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from nn import GraphAttentionNetwork, GraphConvolutionalNetwork, Mlp\n",
    "import wandb\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model,optimizer,epochs,datasetName):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.epochs=epochs\n",
    "        self.loadDataset(datasetName)\n",
    "\n",
    "    #training loop\n",
    "    def train(self):\n",
    "        it=0\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            it+=1    \n",
    "            self.optimizer.zero_grad()    \n",
    "            # if the database is ppi then the task is the multilabeling classification \n",
    "            #otherwise is node classification\n",
    "            if self.datasetName==\"ppi\":\n",
    "                out = model(self.dataset[0])\n",
    "                #the loss in this case is the binary cross entropy with logits loss\n",
    "                #the logits are numbers between -inf and inf not yet normalized for every class\n",
    "                #in this case i have multiple graphs so i just pass the first graph to train on\n",
    "                #i could improve it to train on all the others like in a kfold cross validation\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(out, self.dataset[0].y)\n",
    "            else:\n",
    "                out = model(self.dataset)\n",
    "                #the loss here is the negative log likelihood \n",
    "                #i mask the graph since i have just one graph to train and then i evaluate with a different mask\n",
    "                #i softmax the output to apply the nll loss\n",
    "                out=F.log_softmax(out, dim=1)\n",
    "                loss = F.nll_loss(out[self.dataset.train_mask], self.dataset.y[self.dataset.train_mask])\n",
    "            #backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"mean_loss_train\",loss.data.mean())\n",
    "            wandb.log({\"mean_loss_train\":loss.data.mean()})\n",
    "            #i evaluate the model on the test part of the graph\n",
    "            self.test()\n",
    "\n",
    "    def test(self):\n",
    "        print(\"testing phase\")\n",
    "        model.eval()\n",
    "        #if the dataset is ppi then the task is the multilabeling classification\n",
    "        #so i get the output and then i apply the sigmoid to get the probability of each class\n",
    "        #i threshold the probability to get the class for each label\n",
    "        #then i compare the output with the ground truth\n",
    "        with torch.no_grad():\n",
    "            if self.datasetName==\"ppi\":\n",
    "                #i evaluate the model on the second graph of the dataset not the first of the trainign\n",
    "                pred = model(self.dataset[1])\n",
    "                y=self.dataset[1].y\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(pred, y)\n",
    "                pred=F.sigmoid(pred)\n",
    "                pred=torch.where(pred>0.5,1,0)\n",
    "                correct=(pred==y).sum()\n",
    "                acc=correct/len(y)\n",
    "                # correct = (pred == y).sum()\n",
    "                # # acc=F1Score(121,0)(pred,y)\n",
    "                # acc = int(correct) / int(self.dataset[1].y.shape[0])\n",
    "            else:\n",
    "                #if the dataset is cora, citeseer or pubmed then the task is the node classification\n",
    "                #so i have the nll loss at the end on the log softmax output\n",
    "                pred = model(self.dataset)\n",
    "                #i softmax the prdictions and i apply the negative log likelihood to the test graph (masked)and the argmax to get the class\n",
    "                pred=F.log_softmax(pred, dim=1)\n",
    "                loss = F.nll_loss(pred[self.dataset.test_mask], self.dataset.y[self.dataset.test_mask])\n",
    "                #calculate accuracy\n",
    "                pred=pred.argmax(dim=1)\n",
    "                correct = (pred[self.dataset.test_mask] == self.dataset.y[self.dataset.test_mask]).sum()\n",
    "                acc = int(correct) / int(self.dataset.test_mask.sum())\n",
    "\n",
    "        #log the results\n",
    "        print(\"mean_loss_test\",loss.data.mean(),\"mean_accuracy_test:\",acc)\n",
    "        wandb.log({\"mean_loss_test\":loss.data.mean(),\"mean_accuracy_test\":acc})\n",
    "\n",
    "\n",
    "    #this function loads the dataset based on the name \n",
    "    def loadDataset(self,datasetName=\"cora\"):\n",
    "        self.datasetName = datasetName\n",
    "        #should be cora, citeseer and pubmed or ppi\n",
    "        if datasetName==\"ppi\":\n",
    "            self.dataset=PPI(root=\"./data/ppi\")\n",
    "        else:\n",
    "            self.dataset=Planetoid(name=datasetName,root=\"./data/\"+datasetName)\n",
    "        #print the dataset generalities\n",
    "        print(\"name of dataset: {},number of classes: {},number of nodes features: {},number of graphs: {}\".format(self.dataset,self.dataset.num_classes,self.dataset.num_node_features,len(self.dataset)))\n",
    "        if datasetName!=\"ppi\":\n",
    "            self.dataset=self.dataset[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this is the number of features of the input and the output dimension that the model will have based on the dataset\n",
    "    inOut={\n",
    "        \"cora\":(1433,7),\n",
    "        \"citeseer\":(3703,6),\n",
    "        \"pubmed\":(500,3),\n",
    "        \"ppi\":(50,121)\n",
    "    }\n",
    "    # i set the name of the dataset here and the type of model\n",
    "    datasetName=\"pubmed\"\n",
    "    type=\"gcn\"\n",
    "    device = torch.device('cpu')\n",
    "    #i create the model based on the settings of dataset name and type\n",
    "    if type==\"gcn\":\n",
    "        model=GraphConvolutionalNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"gat\":\n",
    "        model=GraphAttentionNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"mlp\":\n",
    "        model=Mlp(inOut[datasetName][0],50,inOut[datasetName][1])\n",
    "    # i create the the wandb project and the run    \n",
    "    name=type+\" \"+datasetName\n",
    "    run=wandb.init(project='graphAttentionNetwork', entity='bbooss97',name=name)\n",
    "    model.to(device)\n",
    "    run.watch(model)\n",
    "    #define the settings for the training\n",
    "    epochs=200\n",
    "    optmimizer=torch.optim.Adam(model.parameters())\n",
    "    #create the trainer and start the training process\n",
    "    trainer=Trainer(model,optmimizer,epochs,datasetName=datasetName)\n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64d513b1a87d07c6464cdacba50c5010a19b1553f5ba782d4f0aaec460faf0d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
