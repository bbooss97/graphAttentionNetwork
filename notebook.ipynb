{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH ATTENTION NETWORKS\n",
    "\n",
    "The following is the reimplementation and reproduction of results of the paper Graph Attention Networks by Petar Velickovic,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro Li√≤,Yoshua Bengio.\n",
    "\n",
    "What is a graph attention network?\n",
    "It is a neural network architecture that operates on graph data and leveraging on graph convolutional networks and self attention layers allows to improve the accuracy of previous techniques on common graph datasets including the cora dateset,citeseet,pubmed and ppi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph convolutional layer:\n",
    "\n",
    "In the case of graph data the input is a graph with nodes that have a n dimensional vector representing its embeddings and an adiacency matrix representing the connections between the different nodes of the graph.\n",
    "In the case of graph convolutional layer to produce the hidden representation of a node x we would apply a sort of message passing ,where there are parameters to optimize ,between the nodes x and its neighbors, and this operation is done in parallel in all the nodes. \n",
    "The nodes features are all stacked in a matrix and this matrix is used to derive the hidden features for all the nodes using a W matrix of learnable weights and the adiacency matrix.\n",
    "The weight matrix is where the learning takes place and could be described as a matrix or seen as a fully connected layer on the node embeddings.\n",
    "The nodes features are obtained from the summation over all the neighbors features that are derived from the matrix multiplication with the weight matrix\n",
    "The general formula to obtain the hidden features at layer i is:\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Where W is the weight matrix,hj are the node embeddings from the last layer of the neightboor j,N is the set of neighbors of the node i and sigma is an activation function.\n",
    "The aggregation function at the end can be a simple sum or for example an average or a different type of formula even one with learnable parameters.\n",
    "In practise all of those operations are done in parallel using matrix multiplication and gpu for example in the pytorch geometric package and can be summarized as multiplying the features nodes with the weight matrix and then with the adiacency matrix and at the final step apply an activation function.\n",
    "\n",
    "With convolutional layers we obtain a sort of message passing with learning parameters and the messages comes from the neighbors, thats why the number of convolutional layers is important because applying n convolutional layers allows to connect nodes n edges apart and allow them to comunicate with messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of how a convolutional layer is implemented:\n",
    "\n",
    "![alt text](graphconvolutionallayer.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention:\n",
    "\n",
    "The convolutional layer can be estended using self attention.\n",
    "One of the most important features of the graph attention network is the use of self attention, a mechanism commonly used in state of the art models in natural language processing , capable of learning how to give attention to different parts of the inputs to give the best possible results. \n",
    "The node i in this case not only has to work with its neighbors as in the graph convolutional layers but has also to learn attention coefficients to give to the neighboors, to weight the features coming from them.\n",
    "\n",
    "The attention coefficent that nodes i gives to node j can be seen as \n",
    "\\begin{equation}\n",
    "e_{i j}=a\\left(W h_{i}, W h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where W is the parameters matrix and hi and hj are the nodes embeddings.\n",
    "\n",
    "Before describing how the attention function a is implemented lets describe the normalization applied to the coefficients of node i neighboors:\n",
    "this is the formula used to normaliza that is a simple softmax in order to normalize the coefficients of the neighbors to have a value between 0 and 1 and weighted by their importance :\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\operatorname{softmax}_{j}\\left(e_{i j}\\right)=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k e N(i)} \\exp \\left(e_{i k}\\right)}=\\frac{\\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}{\\sum_{k e N(i)} \\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the softmax function firstly exponentiates all the eij attention coefficients to obtain values between 0 and 1 and then returns the percentage of its attention with respect to the attention given to all the neighboors togheter.\n",
    "\n",
    "Now let's see how the attention coefficients aij are calculated.\n",
    "There are different ways to get the attention coefficients, for example in the original paper \"attention is all you need\" its calculated by using the dot product of the features learned applying the a linear trasformation to the embeddings as in the transformers layers.\n",
    "\n",
    "However in this paper they opted for a different type of attention mechanism:\n",
    "\n",
    "they firstly stack the features obtained by multiplying the embeddings with W and then they apply a fully connected layer to get a value that is then passed by a leaky relu activation function before the softmax normalization.\n",
    "In the softmax normalization there is the need to apply a mask matrix in practise in order to normalize only on the neightboors of node i and not include all the others.\n",
    "The formula applied at the end to obtain the coefficients normalized is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\frac{\\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}{\\sum_{k \\in N(i)} \\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the LeakyReLu is used to cut off all the negative values by a large amount but the entire one and is helpful to emphatize positive relationships found by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of the self attention mechanism used in the paper:\n",
    "\n",
    "![alt text](selfattention.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph attention network:\n",
    "\n",
    "merging the things seen before allows to implement the graph attention layer where there is the use of the graph convolutional layer and the attention coefficients to weight the neighbors messages like a linear combination.\n",
    "Here is the formula that describes how to obtain the node features:\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} \\alpha_{i j} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "the aij are values between 0 and 1 that sums to one and are applied in the summation before the activation function\n",
    "\n",
    "To summarize ,the learnable weights are the ones of the matrix W used to transform the embeddings and the ones of the attention mechanism (fully connected layer).\n",
    "In practise we use multihead attention ,so we learn multiple attention coefficients between i and j and we use those to weight the message passing of the neightboors and obtain the features of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of the entire graph attention layer:\n",
    "\n",
    "![alt text](graphattentionlayer.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of how its performed in practise as a recap\n",
    "\n",
    "\n",
    "![alt text](graphattentionlayer2.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the reimplementation of the architecture and the use of it in the datasets cora,citeseer,pubmed and ppi as in the paper.\n",
    "The comments in the following code blocks describe more in details what is done.\n",
    "I created 3 types of neural networks that are a simple mlp, a graph convolutional network and a graph attention network and i tested all of them in the datasets mentioned above.\n",
    "The mlp doesn't use the graph connections to execute the task meanwhile the other 2 networks do.\n",
    "The graph convolutional layers and the graph attention layers are taken from the torch geometric framework and are implemented as matrix multiplication between weights embeddings and adiacency matrix do do operations in parallel ,and the self attention is the one of the paper done with the fully connected layer normalized as in the paper.\n",
    "The datasets are loaded from the pytorch datasets and the tasks are node classification for the datasets cora,citeseer and pubmed and multilabeling classification in the case of ppi.\n",
    "Following the comments all the operations done will be clarified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cora citeseer and pubmed i used a graph and masked it for training and testing.So the testing is done in the same graph but in different nodes not seen in the training loop.In the case of ppi instead the training is done on one graph and the testing is done in another graph.The performances in this case could be improved, training in all the graphs except one and use this remaining graph to test the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used weight and biases to monitor the performance of the networks in the various tasks and to get the report of the results.\n",
    "The results are similar to the one of the paper but not the same.\n",
    "There may be differences in the number of layers used in my case just 2 or in the number of heads and hidden feautures sizes.\n",
    "Also the mlp works well compared to the results of the paper but the architecture i used has a lot of parameters with respect to the gcn and gat so that may be the reason.\n",
    "A lot of different experiments can be done to increase the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the final results:\n",
    "\n",
    "Mean train loss: \n",
    "\n",
    "\n",
    "\n",
    "![alt text](meanlosstrain.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean test loss:\n",
    "\n",
    "\n",
    "![alt text](meanlosstest.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean test accuracy:\n",
    "\n",
    "\n",
    "![alt text](meanaccuracytest.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#this is the graph attention network module\n",
    "class GraphAttentionNetwork(torch.nn.Module):\n",
    "  \"\"\"Graph Attention Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "    super().__init__()\n",
    "    #we have 2 graph attention layers specifying dimension of inputs,outputs and heads\n",
    "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "  def forward(self, data):\n",
    "    #i get the input features and the graph connections as input x\n",
    "    x, edge_index=data.x,data.edge_index\n",
    "    #i filter the input features with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(x, p=0.6, training=self.training)\n",
    "    #i apply the first graph attention layer\n",
    "    h = self.gat1(x, edge_index)\n",
    "    #i apply a elu activation function as in the paper\n",
    "    h = F.elu(h)\n",
    "    #i filter the features h with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(h, p=0.6, training=self.training)\n",
    "    #i apply the second graph attention layer\n",
    "    h = self.gat2(h, edge_index)\n",
    "    #i return the logsoftmax of the output to normalize the output\n",
    "    return h\n",
    "\n",
    "#this is the graph convolutional network module\n",
    "class GraphConvolutionalNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.conv1 = GCNConv(in_size, h_size)\n",
    "        self.conv2 = GCNConv(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.fc1 = torch.nn.Linear(in_size, h_size)\n",
    "        self.fc2 = torch.nn.Linear(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.fc1(x)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.fc2(x)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from nn import GraphAttentionNetwork, GraphConvolutionalNetwork, Mlp\n",
    "import wandb\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model,optimizer,epochs,datasetName):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.epochs=epochs\n",
    "        self.loadDataset(datasetName)\n",
    "\n",
    "    #training loop\n",
    "    def train(self):\n",
    "        it=0\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            it+=1    \n",
    "            self.optimizer.zero_grad()    \n",
    "            # if the database is ppi then the task is the multilabeling classification \n",
    "            #otherwise is node classification\n",
    "            if self.datasetName==\"ppi\":\n",
    "                out = model(self.dataset[0])\n",
    "                #the loss in this case is the binary cross entropy with logits loss\n",
    "                #the logits are numbers between -inf and inf not yet normalized for every class\n",
    "                #in this case i have multiple graphs so i just pass the first graph to train on\n",
    "                #i could improve it to train on all the others like in a kfold cross validation\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(out, self.dataset[0].y)\n",
    "            else:\n",
    "                out = model(self.dataset)\n",
    "                #the loss here is the negative log likelihood \n",
    "                #i mask the graph since i have just one graph to train and then i evaluate with a different mask\n",
    "                #i softmax the output to apply the nll loss\n",
    "                out=F.log_softmax(out, dim=1)\n",
    "                loss = F.nll_loss(out[self.dataset.train_mask], self.dataset.y[self.dataset.train_mask])\n",
    "            #backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"mean_loss_train\",loss.data.mean())\n",
    "            wandb.log({\"mean_loss_train\":loss.data.mean()})\n",
    "            #i evaluate the model on the test part of the graph\n",
    "            self.test()\n",
    "\n",
    "    def test(self):\n",
    "        print(\"testing phase\")\n",
    "        model.eval()\n",
    "        #if the dataset is ppi then the task is the multilabeling classification\n",
    "        #so i get the output and then i apply the sigmoid to get the probability of each class\n",
    "        #i threshold the probability to get the class for each label\n",
    "        #then i compare the output with the ground truth\n",
    "        with torch.no_grad():\n",
    "            if self.datasetName==\"ppi\":\n",
    "                #i evaluate the model on the second graph of the dataset not the first of the trainign\n",
    "                pred = model(self.dataset[1])\n",
    "                y=self.dataset[1].y\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(pred, y)\n",
    "                pred=F.sigmoid(pred)\n",
    "                pred=torch.where(pred>0.5,1,0)\n",
    "                correct=(pred==y).sum()\n",
    "                acc=correct/len(y)\n",
    "                acc/=100\n",
    "                # correct = (pred == y).sum()\n",
    "                # # acc=F1Score(121,0)(pred,y)\n",
    "                # acc = int(correct) / int(self.dataset[1].y.shape[0])\n",
    "            else:\n",
    "                #if the dataset is cora, citeseer or pubmed then the task is the node classification\n",
    "                #so i have the nll loss at the end on the log softmax output\n",
    "                pred = model(self.dataset)\n",
    "                #i softmax the prdictions and i apply the negative log likelihood to the test graph (masked)and the argmax to get the class\n",
    "                pred=F.log_softmax(pred, dim=1)\n",
    "                loss = F.nll_loss(pred[self.dataset.test_mask], self.dataset.y[self.dataset.test_mask])\n",
    "                #calculate accuracy\n",
    "                pred=pred.argmax(dim=1)\n",
    "                correct = (pred[self.dataset.test_mask] == self.dataset.y[self.dataset.test_mask]).sum()\n",
    "                acc = int(correct) / int(self.dataset.test_mask.sum())\n",
    "\n",
    "        #log the results\n",
    "        print(\"mean_loss_test\",loss.data.mean(),\"mean_accuracy_test:\",acc)\n",
    "        wandb.log({\"mean_loss_test\":loss.data.mean(),\"mean_accuracy_test\":acc})\n",
    "\n",
    "\n",
    "    #this function loads the dataset based on the name \n",
    "    def loadDataset(self,datasetName=\"cora\"):\n",
    "        self.datasetName = datasetName\n",
    "        #should be cora, citeseer and pubmed or ppi\n",
    "        if datasetName==\"ppi\":\n",
    "            self.dataset=PPI(root=\"./data/ppi\")\n",
    "        else:\n",
    "            self.dataset=Planetoid(name=datasetName,root=\"./data/\"+datasetName)\n",
    "        #print the dataset generalities\n",
    "        print(\"name of dataset: {},number of classes: {},number of nodes features: {},number of graphs: {}\".format(self.dataset,self.dataset.num_classes,self.dataset.num_node_features,len(self.dataset)))\n",
    "        if datasetName!=\"ppi\":\n",
    "            self.dataset=self.dataset[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this is the number of features of the input and the output dimension that the model will have based on the dataset\n",
    "    inOut={\n",
    "        \"cora\":(1433,7),\n",
    "        \"citeseer\":(3703,6),\n",
    "        \"pubmed\":(500,3),\n",
    "        \"ppi\":(50,121)\n",
    "    }\n",
    "    # i set the name of the dataset here and the type of model\n",
    "    datasetName=\"ppi\"\n",
    "    type=\"gat\"\n",
    "    device = torch.device('cpu')\n",
    "    #i create the model based on the settings of dataset name and type\n",
    "    if type==\"gcn\":\n",
    "        model=GraphConvolutionalNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"gat\":\n",
    "        model=GraphAttentionNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"mlp\":\n",
    "        model=Mlp(inOut[datasetName][0],50,inOut[datasetName][1])\n",
    "    # i create the the wandb project and the run    \n",
    "    name=type+\" \"+datasetName\n",
    "    run=wandb.init(project='graphAttentionNetwork', entity='bbooss97',name=name)\n",
    "    model.to(device)\n",
    "    run.watch(model)\n",
    "    #define the settings for the training\n",
    "    epochs=200\n",
    "    optmimizer=torch.optim.Adam(model.parameters())\n",
    "    #create the trainer and start the training process\n",
    "    trainer=Trainer(model,optmimizer,epochs,datasetName=datasetName)\n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05c73f36370b37d7956a45e693a83b97388709ec0b54883dc202775cd3dcb887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
