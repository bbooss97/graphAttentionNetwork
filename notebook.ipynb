{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH ATTENTION NETWORKS\n",
    "\n",
    "The following is the reimplementation and reproduction of results of the paper Graph Attention Networks by Petar Velickovic,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro Li√≤,Yoshua Bengio.\n",
    "\n",
    "What is a graph attention network?\n",
    "It is a neural network architecture that operates on graph data and leveraging on graph convolutional networks and self attention layers allows to improve the accuracy of previous techniques on common graph datasets including the cora dateset,citeseet,pubmed and ppi.\n",
    "\n",
    "Graph convolutional layer:\n",
    "\n",
    "In the case of graph data the input is a graph with nodes that have a n dimensional vector representing its embeddings and an adiacency matrix representing the connections between the different nodes of the graph.\n",
    "In the case of graph convolutional layer to produce the hidden representation of a node x we would apply a sort of message passing ,where there are parameters to optimize ,between the nodes x and its neighbors, and this operation is done in parallel in all the nodes. \n",
    "The nodes features are all stacked in a matrix and this matrix is used to derive the hidden features for all the nodes using a W matrix of learnable weights and the adiacency matrix.\n",
    "The weight matrix is where the learning takes place and could be described as a matrix or seen as a fully connected layer on the node embeddings.\n",
    "The nodes features are obtained from the summation over all the neighbors features that are derived from the matrix multiplication with the weight matrix\n",
    "The general formula to obtain the hidden features at layer i is:\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Where W is the weight matrix,hj are the node embeddings from the last layer of the neightboor j,N is the set of neighbors of the node i and sigma is an activation function.\n",
    "The aggregation function at the end can be a simple sum or for example an average or a different type of formula even one with learnable parameters.\n",
    "In practise all of those operations are done in parallel using matrix multiplication and gpu for example in the pytorch geometric package and can be summarized as multiplying the features nodes with the weight matrix and then with the adiacency matrix and at the final step apply an activation function.\n",
    "\n",
    "With convolutional layers we obtain a sort of message passing with learning parameters and the messages comes from the neighbors, thats why the number of convolutional layers is important because applying n convolutional layers allows to connect nodes n edges apart and allow them to comunicate with messages.\n",
    "\n",
    "Self attention:\n",
    "\n",
    "The convolutional layer can be estended using self attention.\n",
    "One of the most important features of the graph attention network is the use of self attention, a mechanism commonly used in state of the art models in natural language processing , capable of learning how to give attention to different parts of the inputs to give the best possible results. \n",
    "The node i in this case not only has to work with its neighbors as in the graph convolutional layers but has also to learn attention coefficients to give to the neighboors, to weight the features coming from them.\n",
    "\n",
    "The attention coefficent that nodes i gives to node j can be seen as \n",
    "\\begin{equation}\n",
    "e_{i j}=a\\left(W h_{i}, W h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where W is the parameters matrix and hi and hj are the nodes embeddings.\n",
    "\n",
    "Before describing how the attention function a is implemented lets describe the normalization applied to the coefficients of node i neighboors:\n",
    "this is the formula used to normaliza that is a simple softmax in order to normalize the coefficients of the neighbors to have a value between 0 and 1 and weighted by their importance :\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\operatorname{softmax}_{j}\\left(e_{i j}\\right)=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k e N(i)} \\exp \\left(e_{i k}\\right)}=\\frac{\\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}{\\sum_{k e N(i)} \\exp \\left(a\\left(W h_{i}, W h_{j}\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the softmax function firstly exponentiates all the eij attention coefficients to obtain values between 0 and 1 and then returns the percentage of its attention with respect to the attention given to all the neighboors togheter.\n",
    "\n",
    "Now let's see how the attention coefficients aij are calculated.\n",
    "There are different ways to get the attention coefficients, for example in the original paper \"attention is all you need\" its calculated by using the dot product of the features learned applying the a linear trasformation to the embeddings as in the transformers layers.\n",
    "\n",
    "However in this paper they opted for a different type of attention mechanism:\n",
    "\n",
    "they firstly stack the features obtained by multiplying the embeddings with W and then they apply a fully connected layer to get a value that is then passed by a leaky relu activation function before the softmax normalization.\n",
    "In the softmax normalization there is the need to apply a mask matrix in practise in order to normalize only on the neightboors of node i and not include all the others.\n",
    "The formula applied at the end to obtain the coefficients normalized is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i j}=\\frac{\\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}{\\sum_{k \\in N(i)} \\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{w_{a}^{T}}\\left[W h_{i} \\| W h_{j}\\right]\\right)\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "the LeakyReLu is used to cut off all the negative values by a large amount but the entire one and is helpful to emphatize positive relationships found by the model.\n",
    "\n",
    "graph attention network:\n",
    "\n",
    "merging the things seen before allows to implement the graph attention layer where there is the use of the graph convolutional layer and the attention coefficients to weight the neighbors messages like a linear combination.\n",
    "Here is the formula that describes how to obtain the node features:\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in N(i)} \\alpha_{i j} W * h_{j}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "the aij are values between 0 and 1 that sums to one and are applied in the summation before the activation function\n",
    "\n",
    "To summarize ,the learnable weights are the ones of the matrix W used to transform the embeddings and the ones of the attention mechanism (fully connected layer).\n",
    "In practise we use multihead attention ,so we learn multiple attention coefficients between i and j and we use those to weight the message passing of the neightboors and obtain the features of the output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of the entire graph attention layer:\n",
    "\n",
    "![alt text](graphattentionlayer.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a picture of how its performed in practise\n",
    "\n",
    "\n",
    "![alt text](graphattentionlayer2.png \"Graph attention layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#this is the graph attention network module\n",
    "class GraphAttentionNetwork(torch.nn.Module):\n",
    "  \"\"\"Graph Attention Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "    super().__init__()\n",
    "    #we have 2 graph attention layers specifying dimension of inputs,outputs and heads\n",
    "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "  def forward(self, data):\n",
    "    #i get the input features and the graph connections as input x\n",
    "    x, edge_index=data.x,data.edge_index\n",
    "    #i filter the input features with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(x, p=0.6, training=self.training)\n",
    "    #i apply the first graph attention layer\n",
    "    h = self.gat1(x, edge_index)\n",
    "    #i apply a elu activation function as in the paper\n",
    "    h = F.elu(h)\n",
    "    #i filter the features h with a dropout layer avoid overfitting and improve generalization\n",
    "    h = F.dropout(h, p=0.6, training=self.training)\n",
    "    #i apply the second graph attention layer\n",
    "    h = self.gat2(h, edge_index)\n",
    "    #i return the logsoftmax of the output to normalize the output\n",
    "    return h\n",
    "\n",
    "#this is the graph convolutional network module\n",
    "class GraphConvolutionalNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.conv1 = GCNConv(in_size, h_size)\n",
    "        self.conv2 = GCNConv(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self, in_size, h_size,out_size):\n",
    "        super().__init__()\n",
    "        #we have 2 graph convolutional layers specifying dimension of inputs,outputs and hiddend features\n",
    "        self.fc1 = torch.nn.Linear(in_size, h_size)\n",
    "        self.fc2 = torch.nn.Linear(h_size, out_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #i get the input features and the graph connections as input x\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        #dropout layer to avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the first graph convolutional layer\n",
    "        x = self.fc1(x)\n",
    "        #i apply a relu activation function \n",
    "        x = F.relu(x)\n",
    "         #i filter the features x with a dropout layer avoid overfitting and improve generalization\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        #i apply the second graph convolutional layer\n",
    "        x = self.fc2(x)\n",
    "        #i return the logsoftmax of the output to normalize the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbbooss97\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Andrea\\Desktop\\graphAttentionNetwork\\wandb\\run-20220825_173122-2s8sex4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bbooss97/graphAttentionNetwork/runs/2s8sex4c\" target=\"_blank\">gcn pubmed</a></strong> to <a href=\"https://wandb.ai/bbooss97/graphAttentionNetwork\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name of dataset: pubmed(),number of classes: 3,number of nodes features: 500,number of graphs: 1\n",
      "mean_loss_train tensor(1.1013)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0978) mean_accuracy_test: 0.383\n",
      "mean_loss_train tensor(1.0996)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0975) mean_accuracy_test: 0.4\n",
      "mean_loss_train tensor(1.0983)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0971) mean_accuracy_test: 0.413\n",
      "mean_loss_train tensor(1.0971)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0966) mean_accuracy_test: 0.425\n",
      "mean_loss_train tensor(1.0958)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0960) mean_accuracy_test: 0.442\n",
      "mean_loss_train tensor(1.0945)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0954) mean_accuracy_test: 0.456\n",
      "mean_loss_train tensor(1.0932)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0947) mean_accuracy_test: 0.479\n",
      "mean_loss_train tensor(1.0919)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0939) mean_accuracy_test: 0.504\n",
      "mean_loss_train tensor(1.0906)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0931) mean_accuracy_test: 0.521\n",
      "mean_loss_train tensor(1.0892)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0922) mean_accuracy_test: 0.536\n",
      "mean_loss_train tensor(1.0878)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0912) mean_accuracy_test: 0.554\n",
      "mean_loss_train tensor(1.0863)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0902) mean_accuracy_test: 0.565\n",
      "mean_loss_train tensor(1.0848)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0892) mean_accuracy_test: 0.581\n",
      "mean_loss_train tensor(1.0832)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0881) mean_accuracy_test: 0.591\n",
      "mean_loss_train tensor(1.0815)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0870) mean_accuracy_test: 0.608\n",
      "mean_loss_train tensor(1.0798)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0858) mean_accuracy_test: 0.612\n",
      "mean_loss_train tensor(1.0780)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0846) mean_accuracy_test: 0.626\n",
      "mean_loss_train tensor(1.0762)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0833) mean_accuracy_test: 0.631\n",
      "mean_loss_train tensor(1.0743)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0821) mean_accuracy_test: 0.635\n",
      "mean_loss_train tensor(1.0723)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0808) mean_accuracy_test: 0.642\n",
      "mean_loss_train tensor(1.0703)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0794) mean_accuracy_test: 0.648\n",
      "mean_loss_train tensor(1.0682)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0781) mean_accuracy_test: 0.657\n",
      "mean_loss_train tensor(1.0662)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0766) mean_accuracy_test: 0.658\n",
      "mean_loss_train tensor(1.0640)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0752) mean_accuracy_test: 0.666\n",
      "mean_loss_train tensor(1.0618)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0737) mean_accuracy_test: 0.673\n",
      "mean_loss_train tensor(1.0596)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0722) mean_accuracy_test: 0.677\n",
      "mean_loss_train tensor(1.0573)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0706) mean_accuracy_test: 0.683\n",
      "mean_loss_train tensor(1.0550)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0691) mean_accuracy_test: 0.685\n",
      "mean_loss_train tensor(1.0526)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0675) mean_accuracy_test: 0.687\n",
      "mean_loss_train tensor(1.0502)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0659) mean_accuracy_test: 0.686\n",
      "mean_loss_train tensor(1.0478)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0643) mean_accuracy_test: 0.689\n",
      "mean_loss_train tensor(1.0454)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0627) mean_accuracy_test: 0.693\n",
      "mean_loss_train tensor(1.0430)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0611) mean_accuracy_test: 0.696\n",
      "mean_loss_train tensor(1.0405)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0595) mean_accuracy_test: 0.697\n",
      "mean_loss_train tensor(1.0380)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0579) mean_accuracy_test: 0.697\n",
      "mean_loss_train tensor(1.0356)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0563) mean_accuracy_test: 0.698\n",
      "mean_loss_train tensor(1.0331)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0548) mean_accuracy_test: 0.701\n",
      "mean_loss_train tensor(1.0305)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0532) mean_accuracy_test: 0.702\n",
      "mean_loss_train tensor(1.0280)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0517) mean_accuracy_test: 0.705\n",
      "mean_loss_train tensor(1.0255)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0501) mean_accuracy_test: 0.705\n",
      "mean_loss_train tensor(1.0229)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0486) mean_accuracy_test: 0.706\n",
      "mean_loss_train tensor(1.0203)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0470) mean_accuracy_test: 0.708\n",
      "mean_loss_train tensor(1.0177)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0455) mean_accuracy_test: 0.708\n",
      "mean_loss_train tensor(1.0151)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0440) mean_accuracy_test: 0.709\n",
      "mean_loss_train tensor(1.0124)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0425) mean_accuracy_test: 0.71\n",
      "mean_loss_train tensor(1.0098)\n",
      "testing phase\n",
      "mean_loss_test tensor(1.0409) mean_accuracy_test: 0.709\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andrea\\Desktop\\graphAttentionNetwork\\notebook.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39m#create the trainer and start the training process\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m trainer\u001b[39m=\u001b[39mTrainer(model,optmimizer,epochs,datasetName\u001b[39m=\u001b[39mdatasetName)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32mc:\\Users\\Andrea\\Desktop\\graphAttentionNetwork\\notebook.ipynb Cell 3\u001b[0m in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(out[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtrain_mask], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39my[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtrain_mask])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m#backpropagation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andrea/Desktop/graphAttentionNetwork/notebook.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmean_loss_train\u001b[39m\u001b[39m\"\u001b[39m,loss\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\wandb_torch.py:264\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_tensor_stats(grad\u001b[39m.\u001b[39mdata, name)\n\u001b[1;32m--> 264\u001b[0m handle \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mregister_hook(\u001b[39mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_handles[name] \u001b[39m=\u001b[39m handle\n\u001b[0;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m handle\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from nn import GraphAttentionNetwork, GraphConvolutionalNetwork, Mlp\n",
    "import wandb\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model,optimizer,epochs,datasetName):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.epochs=epochs\n",
    "        self.loadDataset(datasetName)\n",
    "\n",
    "    #training loop\n",
    "    def train(self):\n",
    "        it=0\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            it+=1    \n",
    "            self.optimizer.zero_grad()    \n",
    "            # if the database is ppi then the task is the multilabeling classification \n",
    "            #otherwise is node classification\n",
    "            if self.datasetName==\"ppi\":\n",
    "                out = model(self.dataset[0])\n",
    "                #the loss in this case is the binary cross entropy with logits loss\n",
    "                #the logits are numbers between -inf and inf not yet normalized for every class\n",
    "                #in this case i have multiple graphs so i just pass the first graph to train on\n",
    "                #i could improve it to train on all the others like in a kfold cross validation\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(out, self.dataset[0].y)\n",
    "            else:\n",
    "                out = model(self.dataset)\n",
    "                #the loss here is the negative log likelihood \n",
    "                #i mask the graph since i have just one graph to train and then i evaluate with a different mask\n",
    "                #i softmax the output to apply the nll loss\n",
    "                out=F.log_softmax(out, dim=1)\n",
    "                loss = F.nll_loss(out[self.dataset.train_mask], self.dataset.y[self.dataset.train_mask])\n",
    "            #backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(\"mean_loss_train\",loss.data.mean())\n",
    "            wandb.log({\"mean_loss_train\":loss.data.mean()})\n",
    "            #i evaluate the model on the test part of the graph\n",
    "            self.test()\n",
    "\n",
    "    def test(self):\n",
    "        print(\"testing phase\")\n",
    "        model.eval()\n",
    "        #if the dataset is ppi then the task is the multilabeling classification\n",
    "        #so i get the output and then i apply the sigmoid to get the probability of each class\n",
    "        #i threshold the probability to get the class for each label\n",
    "        #then i compare the output with the ground truth\n",
    "        with torch.no_grad():\n",
    "            if self.datasetName==\"ppi\":\n",
    "                #i evaluate the model on the second graph of the dataset not the first of the trainign\n",
    "                pred = model(self.dataset[1])\n",
    "                y=self.dataset[1].y\n",
    "                loss=torch.nn.BCEWithLogitsLoss(reduction='mean')(pred, y)\n",
    "                pred=F.sigmoid(pred)\n",
    "                pred=torch.where(pred>0.5,1,0)\n",
    "                correct=(pred==y).sum()\n",
    "                acc=correct/len(y)\n",
    "                # correct = (pred == y).sum()\n",
    "                # # acc=F1Score(121,0)(pred,y)\n",
    "                # acc = int(correct) / int(self.dataset[1].y.shape[0])\n",
    "            else:\n",
    "                #if the dataset is cora, citeseer or pubmed then the task is the node classification\n",
    "                #so i have the nll loss at the end on the log softmax output\n",
    "                pred = model(self.dataset)\n",
    "                #i softmax the prdictions and i apply the negative log likelihood to the test graph (masked)and the argmax to get the class\n",
    "                pred=F.log_softmax(pred, dim=1)\n",
    "                loss = F.nll_loss(pred[self.dataset.test_mask], self.dataset.y[self.dataset.test_mask])\n",
    "                #calculate accuracy\n",
    "                pred=pred.argmax(dim=1)\n",
    "                correct = (pred[self.dataset.test_mask] == self.dataset.y[self.dataset.test_mask]).sum()\n",
    "                acc = int(correct) / int(self.dataset.test_mask.sum())\n",
    "\n",
    "        #log the results\n",
    "        print(\"mean_loss_test\",loss.data.mean(),\"mean_accuracy_test:\",acc)\n",
    "        wandb.log({\"mean_loss_test\":loss.data.mean(),\"mean_accuracy_test\":acc})\n",
    "\n",
    "\n",
    "    #this function loads the dataset based on the name \n",
    "    def loadDataset(self,datasetName=\"cora\"):\n",
    "        self.datasetName = datasetName\n",
    "        #should be cora, citeseer and pubmed or ppi\n",
    "        if datasetName==\"ppi\":\n",
    "            self.dataset=PPI(root=\"./data/ppi\")\n",
    "        else:\n",
    "            self.dataset=Planetoid(name=datasetName,root=\"./data/\"+datasetName)\n",
    "        #print the dataset generalities\n",
    "        print(\"name of dataset: {},number of classes: {},number of nodes features: {},number of graphs: {}\".format(self.dataset,self.dataset.num_classes,self.dataset.num_node_features,len(self.dataset)))\n",
    "        if datasetName!=\"ppi\":\n",
    "            self.dataset=self.dataset[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this is the number of features of the input and the output dimension that the model will have based on the dataset\n",
    "    inOut={\n",
    "        \"cora\":(1433,7),\n",
    "        \"citeseer\":(3703,6),\n",
    "        \"pubmed\":(500,3),\n",
    "        \"ppi\":(50,121)\n",
    "    }\n",
    "    # i set the name of the dataset here and the type of model\n",
    "    datasetName=\"pubmed\"\n",
    "    type=\"gcn\"\n",
    "    device = torch.device('cpu')\n",
    "    #i create the model based on the settings of dataset name and type\n",
    "    if type==\"gcn\":\n",
    "        model=GraphConvolutionalNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"gat\":\n",
    "        model=GraphAttentionNetwork(inOut[datasetName][0],13,inOut[datasetName][1])\n",
    "    elif type==\"mlp\":\n",
    "        model=Mlp(inOut[datasetName][0],50,inOut[datasetName][1])\n",
    "    # i create the the wandb project and the run    \n",
    "    name=type+\" \"+datasetName\n",
    "    run=wandb.init(project='graphAttentionNetwork', entity='bbooss97',name=name)\n",
    "    model.to(device)\n",
    "    run.watch(model)\n",
    "    #define the settings for the training\n",
    "    epochs=200\n",
    "    optmimizer=torch.optim.Adam(model.parameters())\n",
    "    #create the trainer and start the training process\n",
    "    trainer=Trainer(model,optmimizer,epochs,datasetName=datasetName)\n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05c73f36370b37d7956a45e693a83b97388709ec0b54883dc202775cd3dcb887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
